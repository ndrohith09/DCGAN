{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as datasets \n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from model_utils import Discriminator , Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.5,), std=(0.5,))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters \n",
    "lr = 0.0005 \n",
    "batch_size = 64 \n",
    "image_size = 64 \n",
    "channels_img = 1 \n",
    "channels_noise = 256\n",
    "num_epochs = 10  \n",
    "\n",
    "# for hoy many times the discriminator and generator is used \n",
    "features_d = 16 \n",
    "features_g = 16 \n",
    "\n",
    "my_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5,) , (0.5,))\n",
    "    ]\n",
    ")\n",
    "my_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST( \n",
    "    root=\"mnist/\" , train=True , transform=my_transforms , download=False\n",
    ") \n",
    "dataloader = DataLoader(dataset , batch_size=batch_size , shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discriminator and generator \n",
    "netD = Discriminator(channels_img , features_d).to(device) \n",
    "netG = Generator(channels_noise , channels_img , features_g).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Generator(\n",
      "  (net): Sequential(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "====================\n",
      "<bound method Module.parameters of Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")>\n",
      "<bound method Module.state_dict of Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")>\n",
      "<bound method Module.to_empty of Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(netD)\n",
    "print(netG)\n",
    "print(\"====================\")\n",
    "print(netD.parameters)\n",
    "print(netD.state_dict)\n",
    "print(netD.to_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Optimizer for G and D\n",
    "optimizerD = optim.Adam(netD.parameters() , lr=lr , betas=(0.5 , 0.999)) \n",
    "optimizerG = optim.Adam(netG.parameters() , lr=lr , betas=(0.5 , 0.999)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "net.0.weight \t torch.Size([16, 1, 4, 4])\n",
      "net.0.bias \t torch.Size([16])\n",
      "net.2.weight \t torch.Size([32, 16, 4, 4])\n",
      "net.2.bias \t torch.Size([32])\n",
      "net.3.weight \t torch.Size([32])\n",
      "net.3.bias \t torch.Size([32])\n",
      "net.3.running_mean \t torch.Size([32])\n",
      "net.3.running_var \t torch.Size([32])\n",
      "net.3.num_batches_tracked \t torch.Size([])\n",
      "net.5.weight \t torch.Size([64, 32, 4, 4])\n",
      "net.5.bias \t torch.Size([64])\n",
      "net.6.weight \t torch.Size([64])\n",
      "net.6.bias \t torch.Size([64])\n",
      "net.6.running_mean \t torch.Size([64])\n",
      "net.6.running_var \t torch.Size([64])\n",
      "net.6.num_batches_tracked \t torch.Size([])\n",
      "net.8.weight \t torch.Size([128, 64, 4, 4])\n",
      "net.8.bias \t torch.Size([128])\n",
      "net.9.weight \t torch.Size([128])\n",
      "net.9.bias \t torch.Size([128])\n",
      "net.9.running_mean \t torch.Size([128])\n",
      "net.9.running_var \t torch.Size([128])\n",
      "net.9.num_batches_tracked \t torch.Size([])\n",
      "net.11.weight \t torch.Size([1, 128, 4, 4])\n",
      "net.11.bias \t torch.Size([1])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0005, 'betas': (0.5, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]\n"
     ]
    }
   ],
   "source": [
    "# Access the model and optimizer state_dict\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in netD.state_dict():\n",
    "    print(param_tensor, \"\\t\", netD.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizerD.state_dict():\n",
    "    print(var_name, \"\\t\", optimizerD.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "net.0.weight \t torch.Size([256, 256, 4, 4])\n",
      "net.0.bias \t torch.Size([256])\n",
      "net.1.weight \t torch.Size([256])\n",
      "net.1.bias \t torch.Size([256])\n",
      "net.1.running_mean \t torch.Size([256])\n",
      "net.1.running_var \t torch.Size([256])\n",
      "net.1.num_batches_tracked \t torch.Size([])\n",
      "net.3.weight \t torch.Size([256, 128, 4, 4])\n",
      "net.3.bias \t torch.Size([128])\n",
      "net.4.weight \t torch.Size([128])\n",
      "net.4.bias \t torch.Size([128])\n",
      "net.4.running_mean \t torch.Size([128])\n",
      "net.4.running_var \t torch.Size([128])\n",
      "net.4.num_batches_tracked \t torch.Size([])\n",
      "net.6.weight \t torch.Size([128, 64, 4, 4])\n",
      "net.6.bias \t torch.Size([64])\n",
      "net.7.weight \t torch.Size([64])\n",
      "net.7.bias \t torch.Size([64])\n",
      "net.7.running_mean \t torch.Size([64])\n",
      "net.7.running_var \t torch.Size([64])\n",
      "net.7.num_batches_tracked \t torch.Size([])\n",
      "net.9.weight \t torch.Size([64, 32, 4, 4])\n",
      "net.9.bias \t torch.Size([32])\n",
      "net.10.weight \t torch.Size([32])\n",
      "net.10.bias \t torch.Size([32])\n",
      "net.10.running_mean \t torch.Size([32])\n",
      "net.10.running_var \t torch.Size([32])\n",
      "net.10.num_batches_tracked \t torch.Size([])\n",
      "net.12.weight \t torch.Size([32, 1, 4, 4])\n",
      "net.12.bias \t torch.Size([1])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0005, 'betas': (0.5, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]}]\n"
     ]
    }
   ],
   "source": [
    "# Access the model and optimizer state_dict\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in netG.state_dict():\n",
    "    print(param_tensor, \"\\t\", netG.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizerG.state_dict():\n",
    "    print(var_name, \"\\t\", optimizerG.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DCGANS.pth\" \n",
    "torch.save({\n",
    "    \"model_state_dictD\" : netD.state_dict(), \n",
    "    \"optimizer_state_dictD\" : optimizerD.state_dict(),\n",
    "    \"model_state_dictG\" : netG.state_dict(), \n",
    "    \"optimizer_state_dictG\" : optimizerG.state_dict() \n",
    "} , PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_netD = Discriminator(channels_img , features_d).to(device) \n",
    "loaded_netG = Generator(channels_noise , channels_img , features_g).to(device)\n",
    "loaded_optimizerD = optim.Adam(loaded_netD.parameters() , lr=lr , betas=(0.5 , 0.999)) \n",
    "loaded_optimizerG = optim.Adam(loaded_netG.parameters() , lr=lr , betas=(0.5 , 0.999)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DCGANS.pth\" \n",
    "checkpoint = torch.load(PATH) \n",
    "device = torch.device(\"cuda\")\n",
    "loaded_netD.load_state_dict(checkpoint[\"model_state_dictD\"])\n",
    "loaded_netG.load_state_dict(checkpoint[\"model_state_dictG\"])\n",
    "loaded_optimizerD.load_state_dict(checkpoint[\"optimizer_state_dictD\"]) \n",
    "loaded_optimizerG.load_state_dict(checkpoint[\"optimizer_state_dictG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2)\n",
      "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2)\n",
      "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2)\n",
      "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (net): Sequential(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_netD.eval())\n",
    "print()\n",
    "print(loaded_netG.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.5, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_optimizerD)\n",
    "print()\n",
    "print(loaded_optimizerG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_netD.train()\n",
    "loaded_netG.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() \n",
    "real_label =1 \n",
    "fake_label = 0\n",
    "\n",
    "fixed_noise = torch.randn(64 , channels_noise , 1 , 1).to(device=device) \n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/test_real\") \n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/test_fake\")  \n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10]\n",
      " Batch [0/938]\n",
      " Loss D : 1.6801 , loss G : 0.9684 D(x) : 0.8062\n",
      "Epoch [0/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.8226 , loss G : 2.8311 D(x) : 0.6771\n",
      "Epoch [0/10]\n",
      " Batch [200/938]\n",
      " Loss D : 1.0125 , loss G : 1.8210 D(x) : 0.7029\n",
      "Epoch [0/10]\n",
      " Batch [300/938]\n",
      " Loss D : 1.1409 , loss G : 1.8153 D(x) : 0.8363\n",
      "Epoch [0/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.9591 , loss G : 1.1022 D(x) : 0.6452\n",
      "Epoch [0/10]\n",
      " Batch [500/938]\n",
      " Loss D : 1.0623 , loss G : 0.9418 D(x) : 0.5143\n",
      "Epoch [0/10]\n",
      " Batch [600/938]\n",
      " Loss D : 1.0656 , loss G : 1.2515 D(x) : 0.7072\n",
      "Epoch [0/10]\n",
      " Batch [700/938]\n",
      " Loss D : 1.0725 , loss G : 1.1241 D(x) : 0.5796\n",
      "Epoch [0/10]\n",
      " Batch [800/938]\n",
      " Loss D : 1.4796 , loss G : 3.5061 D(x) : 0.9161\n",
      "Epoch [0/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.9189 , loss G : 1.4611 D(x) : 0.7424\n",
      "Epoch [1/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.9991 , loss G : 1.1838 D(x) : 0.6449\n",
      "Epoch [1/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.8582 , loss G : 1.3547 D(x) : 0.6976\n",
      "Epoch [1/10]\n",
      " Batch [200/938]\n",
      " Loss D : 0.8539 , loss G : 1.7345 D(x) : 0.8066\n",
      "Epoch [1/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.7995 , loss G : 1.3907 D(x) : 0.7077\n",
      "Epoch [1/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.9775 , loss G : 0.6649 D(x) : 0.5653\n",
      "Epoch [1/10]\n",
      " Batch [500/938]\n",
      " Loss D : 1.0454 , loss G : 2.5781 D(x) : 0.8946\n",
      "Epoch [1/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.9165 , loss G : 1.4269 D(x) : 0.7437\n",
      "Epoch [1/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.9511 , loss G : 1.9677 D(x) : 0.8486\n",
      "Epoch [1/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.9636 , loss G : 1.8664 D(x) : 0.7494\n",
      "Epoch [1/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.8110 , loss G : 1.7509 D(x) : 0.6906\n",
      "Epoch [2/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.9837 , loss G : 1.1438 D(x) : 0.5581\n",
      "Epoch [2/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.7910 , loss G : 1.8498 D(x) : 0.7818\n",
      "Epoch [2/10]\n",
      " Batch [200/938]\n",
      " Loss D : 1.0536 , loss G : 1.6370 D(x) : 0.5697\n",
      "Epoch [2/10]\n",
      " Batch [300/938]\n",
      " Loss D : 1.0143 , loss G : 1.0204 D(x) : 0.5388\n",
      "Epoch [2/10]\n",
      " Batch [400/938]\n",
      " Loss D : 1.1349 , loss G : 1.8198 D(x) : 0.4737\n",
      "Epoch [2/10]\n",
      " Batch [500/938]\n",
      " Loss D : 0.8819 , loss G : 1.8303 D(x) : 0.7129\n",
      "Epoch [2/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.9445 , loss G : 1.1474 D(x) : 0.6010\n",
      "Epoch [2/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.9649 , loss G : 2.7265 D(x) : 0.9109\n",
      "Epoch [2/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.7919 , loss G : 2.9014 D(x) : 0.8939\n",
      "Epoch [2/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7487 , loss G : 1.6760 D(x) : 0.8071\n",
      "Epoch [3/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.7839 , loss G : 1.0466 D(x) : 0.7225\n",
      "Epoch [3/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.8694 , loss G : 1.7642 D(x) : 0.7368\n",
      "Epoch [3/10]\n",
      " Batch [200/938]\n",
      " Loss D : 1.9607 , loss G : 1.8245 D(x) : 0.1919\n",
      "Epoch [3/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.7763 , loss G : 2.7999 D(x) : 0.9040\n",
      "Epoch [3/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6986 , loss G : 2.0965 D(x) : 0.8502\n",
      "Epoch [3/10]\n",
      " Batch [500/938]\n",
      " Loss D : 0.9330 , loss G : 3.4662 D(x) : 0.9413\n",
      "Epoch [3/10]\n",
      " Batch [600/938]\n",
      " Loss D : 1.0795 , loss G : 3.7368 D(x) : 0.9208\n",
      "Epoch [3/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.7801 , loss G : 2.3765 D(x) : 0.8605\n",
      "Epoch [3/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.7502 , loss G : 2.4245 D(x) : 0.7527\n",
      "Epoch [3/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7806 , loss G : 2.0930 D(x) : 0.7306\n",
      "Epoch [4/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.9618 , loss G : 1.3974 D(x) : 0.5770\n",
      "Epoch [4/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.8098 , loss G : 2.6010 D(x) : 0.8939\n",
      "Epoch [4/10]\n",
      " Batch [200/938]\n",
      " Loss D : 0.7324 , loss G : 2.3285 D(x) : 0.7951\n",
      "Epoch [4/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.8754 , loss G : 2.0332 D(x) : 0.8568\n",
      "Epoch [4/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.7157 , loss G : 2.5453 D(x) : 0.9165\n",
      "Epoch [4/10]\n",
      " Batch [500/938]\n",
      " Loss D : 0.8458 , loss G : 3.0845 D(x) : 0.9035\n",
      "Epoch [4/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.8544 , loss G : 2.1168 D(x) : 0.7622\n",
      "Epoch [4/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.8463 , loss G : 2.4421 D(x) : 0.9065\n",
      "Epoch [4/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.7382 , loss G : 3.2343 D(x) : 0.9188\n",
      "Epoch [4/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.6992 , loss G : 2.2726 D(x) : 0.8391\n",
      "Epoch [5/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.7455 , loss G : 3.0776 D(x) : 0.8820\n",
      "Epoch [5/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.6842 , loss G : 2.0442 D(x) : 0.8597\n",
      "Epoch [5/10]\n",
      " Batch [200/938]\n",
      " Loss D : 0.7007 , loss G : 2.1320 D(x) : 0.8266\n",
      "Epoch [5/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6775 , loss G : 2.5912 D(x) : 0.8656\n",
      "Epoch [5/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6895 , loss G : 2.4619 D(x) : 0.8531\n",
      "Epoch [5/10]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6944 , loss G : 2.4052 D(x) : 0.8699\n",
      "Epoch [5/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.7156 , loss G : 2.2232 D(x) : 0.7917\n",
      "Epoch [5/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.9901 , loss G : 1.1387 D(x) : 0.6185\n",
      "Epoch [5/10]\n",
      " Batch [800/938]\n",
      " Loss D : 1.2355 , loss G : 3.1183 D(x) : 0.9253\n",
      "Epoch [5/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7156 , loss G : 2.5960 D(x) : 0.9119\n",
      "Epoch [6/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6810 , loss G : 2.7086 D(x) : 0.9307\n",
      "Epoch [6/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.7403 , loss G : 1.6089 D(x) : 0.7705\n",
      "Epoch [6/10]\n",
      " Batch [200/938]\n",
      " Loss D : 0.7173 , loss G : 1.9368 D(x) : 0.8137\n",
      "Epoch [6/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.8184 , loss G : 1.8017 D(x) : 0.7753\n",
      "Epoch [6/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.7289 , loss G : 1.6012 D(x) : 0.7773\n",
      "Epoch [6/10]\n",
      " Batch [500/938]\n",
      " Loss D : 1.1230 , loss G : 1.2210 D(x) : 0.5075\n",
      "Epoch [6/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.7229 , loss G : 1.8122 D(x) : 0.7827\n",
      "Epoch [6/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6847 , loss G : 2.6033 D(x) : 0.8971\n",
      "Epoch [6/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6672 , loss G : 2.3964 D(x) : 0.9115\n",
      "Epoch [6/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7528 , loss G : 2.9606 D(x) : 0.9242\n",
      "Epoch [7/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.8336 , loss G : 3.0986 D(x) : 0.9494\n",
      "Epoch [7/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.7130 , loss G : 2.5089 D(x) : 0.9221\n",
      "Epoch [7/10]\n",
      " Batch [200/938]\n",
      " Loss D : 0.8249 , loss G : 3.3989 D(x) : 0.9620\n",
      "Epoch [7/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6779 , loss G : 2.0779 D(x) : 0.8991\n",
      "Epoch [7/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.7463 , loss G : 2.1101 D(x) : 0.7952\n",
      "Epoch [7/10]\n",
      " Batch [500/938]\n",
      " Loss D : 1.1162 , loss G : 0.7587 D(x) : 0.5601\n",
      "Epoch [7/10]\n",
      " Batch [600/938]\n",
      " Loss D : 1.0862 , loss G : 3.4933 D(x) : 0.9605\n",
      "Epoch [7/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6733 , loss G : 2.1335 D(x) : 0.8689\n",
      "Epoch [7/10]\n",
      " Batch [800/938]\n",
      " Loss D : 1.0708 , loss G : 1.3974 D(x) : 0.6397\n",
      "Epoch [7/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.6711 , loss G : 2.3881 D(x) : 0.9012\n",
      "Epoch [8/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6818 , loss G : 2.6603 D(x) : 0.9307\n",
      "Epoch [8/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.7534 , loss G : 2.7885 D(x) : 0.9476\n",
      "Epoch [8/10]\n",
      " Batch [200/938]\n",
      " Loss D : 1.8757 , loss G : 4.0324 D(x) : 0.9696\n",
      "Epoch [8/10]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6813 , loss G : 2.1378 D(x) : 0.8573\n",
      "Epoch [8/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6769 , loss G : 2.4684 D(x) : 0.9222\n",
      "Epoch [8/10]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6837 , loss G : 2.1025 D(x) : 0.8641\n",
      "Epoch [8/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.9100 , loss G : 0.8752 D(x) : 0.6028\n",
      "Epoch [8/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6991 , loss G : 2.2916 D(x) : 0.8336\n",
      "Epoch [8/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6710 , loss G : 2.1258 D(x) : 0.8632\n",
      "Epoch [8/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7655 , loss G : 1.4169 D(x) : 0.7406\n",
      "Epoch [9/10]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6927 , loss G : 2.6940 D(x) : 0.9083\n",
      "Epoch [9/10]\n",
      " Batch [100/938]\n",
      " Loss D : 0.6732 , loss G : 2.2225 D(x) : 0.8609\n",
      "Epoch [9/10]\n",
      " Batch [200/938]\n",
      " Loss D : 0.9838 , loss G : 1.6248 D(x) : 0.6424\n",
      "Epoch [9/10]\n",
      " Batch [300/938]\n",
      " Loss D : 1.3161 , loss G : 0.7062 D(x) : 0.3824\n",
      "Epoch [9/10]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6747 , loss G : 2.3191 D(x) : 0.8843\n",
      "Epoch [9/10]\n",
      " Batch [500/938]\n",
      " Loss D : 0.8239 , loss G : 2.3231 D(x) : 0.8283\n",
      "Epoch [9/10]\n",
      " Batch [600/938]\n",
      " Loss D : 0.6761 , loss G : 2.6794 D(x) : 0.9026\n",
      "Epoch [9/10]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6718 , loss G : 1.9378 D(x) : 0.8629\n",
      "Epoch [9/10]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6637 , loss G : 2.4251 D(x) : 0.9129\n",
      "Epoch [9/10]\n",
      " Batch [900/938]\n",
      " Loss D : 0.6765 , loss G : 2.8193 D(x) : 0.8996\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    for batch_idx , (data , targets) in enumerate(dataloader):\n",
    "        data = data.to(device) \n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        # train discriminator : max log(D(x)) + log(1 - D(G(z))) \n",
    "        loaded_netD.zero_grad() \n",
    "        label = (torch.ones(batch_size) * 0.9).to(device) \n",
    "        output = loaded_netD(data).reshape(-1) \n",
    "        lossD_real = criterion(output , label) \n",
    "        D_x = output.mean().item() \n",
    "\n",
    "        noise = torch.randn(batch_size , channels_noise , 1 ,1).to(device) \n",
    "        fake = loaded_netG(noise) \n",
    "        label = (torch.ones(batch_size) * 0.1).to(device)\n",
    "\n",
    "        output = loaded_netD(fake.detach()).reshape(-1) \n",
    "        lossD_fake = criterion(output , label) \n",
    "\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        lossD.backward() \n",
    "        loaded_optimizerD.step() \n",
    "\n",
    "        # train generator : max log(D(G(z))) \n",
    "        loaded_netG.zero_grad() \n",
    "        label = torch.ones(batch_size).to(device) \n",
    "        output = loaded_netD(fake).reshape(-1) \n",
    "        lossG = criterion(output , label) \n",
    "        lossG.backward() \n",
    "        loaded_optimizerG.step()  \n",
    "\n",
    "        # print losses ocassionally and print to tensorboard \n",
    "        if batch_idx % 100 == 0: \n",
    "            step += 1 \n",
    "            print( \n",
    "                f\"Epoch [{epoch}/{num_epochs}]\\n Batch [{batch_idx}/{len(dataloader)}]\\n Loss D : {lossD:.4f} , loss G : {lossG:.4f} D(x) : {D_x:.4f}\" \n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = loaded_netG(fixed_noise) \n",
    "                img_grid_real = torchvision.utils.make_grid(data[:32] , normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32] , normalize=True) \n",
    "                writer_real.add_image(\n",
    "                    \"MNIST Real Image\" , img_grid_real , global_step=step\n",
    "                )\n",
    "                writer_fake.add_image(\n",
    "                    \"MNIST Fake Image\" , img_grid_fake , global_step=step\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2)\n",
       "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_netD.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"DCGANS.pth\"\n",
    "torch.save({\n",
    "    \"model_state_dictD\" : loaded_netD.state_dict(), \n",
    "    \"optimizer_state_dictD\" : loaded_optimizerD.state_dict(),\n",
    "    \"model_state_dictG\" : loaded_netG.state_dict(), \n",
    "    \"optimizer_state_dictG\" : loaded_optimizerG.state_dict() \n",
    "} , PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the saved model and resume the training again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH) \n",
    "device = torch.device(\"cuda\")\n",
    "loaded_netD.load_state_dict(checkpoint[\"model_state_dictD\"])\n",
    "loaded_netG.load_state_dict(checkpoint[\"model_state_dictG\"])\n",
    "loaded_optimizerD.load_state_dict(checkpoint[\"optimizer_state_dictD\"]) \n",
    "loaded_optimizerG.load_state_dict(checkpoint[\"optimizer_state_dictG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() \n",
    "real_label =1 \n",
    "fake_label = 0\n",
    "\n",
    "fixed_noise = torch.randn(64 , channels_noise , 1 , 1).to(device=device) \n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/test_real\") \n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/test_fake\")  \n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6636 , loss G : 2.3946 D(x) : 0.9013\n",
      "Epoch [0/5]\n",
      " Batch [100/938]\n",
      " Loss D : 0.6770 , loss G : 2.4668 D(x) : 0.9242\n",
      "Epoch [0/5]\n",
      " Batch [200/938]\n",
      " Loss D : 0.6897 , loss G : 2.3000 D(x) : 0.8605\n",
      "Epoch [0/5]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6656 , loss G : 2.6390 D(x) : 0.8963\n",
      "Epoch [0/5]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6693 , loss G : 2.1864 D(x) : 0.8631\n",
      "Epoch [0/5]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6971 , loss G : 2.8882 D(x) : 0.9380\n",
      "Epoch [0/5]\n",
      " Batch [600/938]\n",
      " Loss D : 0.6704 , loss G : 1.8668 D(x) : 0.8578\n",
      "Epoch [0/5]\n",
      " Batch [700/938]\n",
      " Loss D : 0.9962 , loss G : 3.1742 D(x) : 0.8996\n",
      "Epoch [0/5]\n",
      " Batch [800/938]\n",
      " Loss D : 0.8058 , loss G : 2.8210 D(x) : 0.9100\n",
      "Epoch [0/5]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7034 , loss G : 2.4620 D(x) : 0.8937\n",
      "Epoch [1/5]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6880 , loss G : 1.6849 D(x) : 0.8296\n",
      "Epoch [1/5]\n",
      " Batch [100/938]\n",
      " Loss D : 0.6873 , loss G : 2.1649 D(x) : 0.8431\n",
      "Epoch [1/5]\n",
      " Batch [200/938]\n",
      " Loss D : 2.0462 , loss G : 0.2729 D(x) : 0.1699\n",
      "Epoch [1/5]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6971 , loss G : 2.7455 D(x) : 0.9407\n",
      "Epoch [1/5]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6727 , loss G : 2.3523 D(x) : 0.9017\n",
      "Epoch [1/5]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6728 , loss G : 2.3380 D(x) : 0.8656\n",
      "Epoch [1/5]\n",
      " Batch [600/938]\n",
      " Loss D : 0.6680 , loss G : 2.3301 D(x) : 0.8702\n",
      "Epoch [1/5]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6755 , loss G : 2.1351 D(x) : 0.8625\n",
      "Epoch [1/5]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6833 , loss G : 2.2890 D(x) : 0.8799\n",
      "Epoch [1/5]\n",
      " Batch [900/938]\n",
      " Loss D : 0.6707 , loss G : 2.2625 D(x) : 0.8762\n",
      "Epoch [2/5]\n",
      " Batch [0/938]\n",
      " Loss D : 0.7019 , loss G : 2.6179 D(x) : 0.9130\n",
      "Epoch [2/5]\n",
      " Batch [100/938]\n",
      " Loss D : 0.9358 , loss G : 3.0314 D(x) : 0.8543\n",
      "Epoch [2/5]\n",
      " Batch [200/938]\n",
      " Loss D : 0.6795 , loss G : 1.7636 D(x) : 0.8367\n",
      "Epoch [2/5]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6661 , loss G : 2.3990 D(x) : 0.8844\n",
      "Epoch [2/5]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6906 , loss G : 2.2749 D(x) : 0.8504\n",
      "Epoch [2/5]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6744 , loss G : 2.3739 D(x) : 0.9104\n",
      "Epoch [2/5]\n",
      " Batch [600/938]\n",
      " Loss D : 0.6836 , loss G : 2.3404 D(x) : 0.8689\n",
      "Epoch [2/5]\n",
      " Batch [700/938]\n",
      " Loss D : 0.7908 , loss G : 2.5924 D(x) : 0.8179\n",
      "Epoch [2/5]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6691 , loss G : 2.3466 D(x) : 0.8916\n",
      "Epoch [2/5]\n",
      " Batch [900/938]\n",
      " Loss D : 0.6790 , loss G : 2.3214 D(x) : 0.8544\n",
      "Epoch [3/5]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6631 , loss G : 2.5357 D(x) : 0.9050\n",
      "Epoch [3/5]\n",
      " Batch [100/938]\n",
      " Loss D : 0.6611 , loss G : 2.3597 D(x) : 0.9112\n",
      "Epoch [3/5]\n",
      " Batch [200/938]\n",
      " Loss D : 0.6770 , loss G : 2.5117 D(x) : 0.9147\n",
      "Epoch [3/5]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6655 , loss G : 2.2663 D(x) : 0.8978\n",
      "Epoch [3/5]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6897 , loss G : 2.5192 D(x) : 0.9377\n",
      "Epoch [3/5]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6995 , loss G : 2.9784 D(x) : 0.9404\n",
      "Epoch [3/5]\n",
      " Batch [600/938]\n",
      " Loss D : 0.7114 , loss G : 2.1346 D(x) : 0.8381\n",
      "Epoch [3/5]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6644 , loss G : 2.1573 D(x) : 0.8832\n",
      "Epoch [3/5]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6652 , loss G : 2.1823 D(x) : 0.8741\n",
      "Epoch [3/5]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7046 , loss G : 1.9998 D(x) : 0.8115\n",
      "Epoch [4/5]\n",
      " Batch [0/938]\n",
      " Loss D : 0.6744 , loss G : 2.2762 D(x) : 0.8702\n",
      "Epoch [4/5]\n",
      " Batch [100/938]\n",
      " Loss D : 0.6766 , loss G : 2.6930 D(x) : 0.8904\n",
      "Epoch [4/5]\n",
      " Batch [200/938]\n",
      " Loss D : 0.8122 , loss G : 2.2103 D(x) : 0.7137\n",
      "Epoch [4/5]\n",
      " Batch [300/938]\n",
      " Loss D : 0.6658 , loss G : 2.4406 D(x) : 0.9125\n",
      "Epoch [4/5]\n",
      " Batch [400/938]\n",
      " Loss D : 0.6683 , loss G : 2.2729 D(x) : 0.9137\n",
      "Epoch [4/5]\n",
      " Batch [500/938]\n",
      " Loss D : 0.6632 , loss G : 2.3826 D(x) : 0.9124\n",
      "Epoch [4/5]\n",
      " Batch [600/938]\n",
      " Loss D : 0.6611 , loss G : 2.4684 D(x) : 0.8987\n",
      "Epoch [4/5]\n",
      " Batch [700/938]\n",
      " Loss D : 0.6636 , loss G : 2.1116 D(x) : 0.8641\n",
      "Epoch [4/5]\n",
      " Batch [800/938]\n",
      " Loss D : 0.6722 , loss G : 2.4386 D(x) : 0.9019\n",
      "Epoch [4/5]\n",
      " Batch [900/938]\n",
      " Loss D : 0.7857 , loss G : 1.6160 D(x) : 0.7198\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters \n",
    "lr = 0.0005  \n",
    "image_size = 64 \n",
    "channels_img = 1 \n",
    "channels_noise = 256\n",
    "new_epoch = 5 \n",
    "batch_size = 64 \n",
    "num_epochs = new_epoch \n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for batch_idx , (data , targets) in enumerate(dataloader):\n",
    "        data = data.to(device) \n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        # train discriminator : max log(D(x)) + log(1 - D(G(z))) \n",
    "        loaded_netD.zero_grad() \n",
    "        label = (torch.ones(batch_size) * 0.9).to(device) \n",
    "        output = loaded_netD(data).reshape(-1) \n",
    "        lossD_real = criterion(output , label) \n",
    "        D_x = output.mean().item() \n",
    "\n",
    "        noise = torch.randn(batch_size , channels_noise , 1 ,1).to(device) \n",
    "        fake = loaded_netG(noise) \n",
    "        label = (torch.ones(batch_size) * 0.1).to(device)\n",
    "\n",
    "        output = loaded_netD(fake.detach()).reshape(-1) \n",
    "        lossD_fake = criterion(output , label) \n",
    "\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        lossD.backward() \n",
    "        loaded_optimizerD.step() \n",
    "\n",
    "        # train generator : max log(D(G(z))) \n",
    "        loaded_netG.zero_grad() \n",
    "        label = torch.ones(batch_size).to(device) \n",
    "        output = loaded_netD(fake).reshape(-1) \n",
    "        lossG = criterion(output , label) \n",
    "        lossG.backward() \n",
    "        loaded_optimizerG.step()  \n",
    "\n",
    "        # print losses ocassionally and print to tensorboard \n",
    "        if batch_idx % 100 == 0: \n",
    "            step += 1 \n",
    "            print( \n",
    "                f\"Epoch [{epoch}/{num_epochs}]\\n Batch [{batch_idx}/{len(dataloader)}]\\n Loss D : {lossD:.4f} , loss G : {lossG:.4f} D(x) : {D_x:.4f}\" \n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = loaded_netG(fixed_noise) \n",
    "                img_grid_real = torchvision.utils.make_grid(data[:32] , normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32] , normalize=True) \n",
    "                writer_real.add_image(\n",
    "                    \"MNIST Real Image\" , img_grid_real , global_step=step\n",
    "                )\n",
    "                writer_fake.add_image(\n",
    "                    \"MNIST Fake Image\" , img_grid_fake , global_step=step\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis \n",
    "\n",
    "*1st training*\n",
    "\n",
    "- [ Epoch 0 ] : Loss D : 0.9189 , loss G : 1.4611 D(x) : 0.7424 \n",
    "- [ Epoch 1 ] : Loss D : 0.8110 , loss G : 1.7509 D(x) : 0.6906\n",
    "- [ Epoch 2 ] : Loss D : 0.7487 , loss G : 1.6760 D(x) : 0.8071\n",
    "- [ Epoch 3 ] : Loss D : 0.7806 , loss G : 2.0930 D(x) : 0.7306\n",
    "- [ Epoch 4 ] : Loss D : 0.6992 , loss G : 2.2726 D(x) : 0.8391\n",
    "- [ Epoch 5 ] : Loss D : 0.7156 , loss G : 2.5960 D(x) : 0.9119\n",
    "- [ Epoch 6 ] : Loss D : 0.7528 , loss G : 2.9606 D(x) : 0.9242\n",
    "- [ Epoch 7 ] : Loss D : 0.6711 , loss G : 2.3881 D(x) : 0.9012\n",
    "- [ Epoch 8 ] : Loss D : 0.7655 , loss G : 1.4169 D(x) : 0.7406\n",
    "- [ Epoch 9 ] : Loss D : 0.6765 , loss G : 2.8193 D(x) : 0.8996  \n",
    "\n",
    "*2nd training*\n",
    "\n",
    "- [Epoch 1] : Loss D : 0.7034 , loss G : 2.4620 D(x) : 0.8937\n",
    "- [Epoch 2] : Loss D : 0.6707 , loss G : 2.2625 D(x) : 0.8762\n",
    "- [Epoch 3] : Loss D : 0.6790 , loss G : 2.3214 D(x) : 0.8544\n",
    "- [Epoch 4] : Loss D : 0.7046 , loss G : 1.9998 D(x) : 0.8115\n",
    "- [Epoch 5] : Loss D : 0.7857 , loss G : 1.6160 D(x) : 0.7198"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
